
# NCME 2022 Training Session

# Data Validation and Analysis in the Era of COVID-19

In this full-day session, participants will be introduced to a comprehensive suite of R based tools that can be used to
address numerous education assessment data analysis and validation issues that arose due the the pandemic. One of the
consequences of the disrupted education due to the pandemic has been cancellation, interruption and modification of the
educational assessment of students. For example, in spring 2020, just after the pandemic began in the United States, all
state summative testing was cancelled after the federal government issues assessment waivers. Similarly, as student education
took place remotely, interim assessment providers have altered their products to allow students to take tests while at home.
These and other alterations to standard testing protocol present unique challenges to psyshometricians and data analysts who validate
and use these data.

Several practical issues emerged due to the pandemic that will be focal topics of instruction during this training session:

## Analysis of Skip-Year Data

Due to the disruption of state assessment in spring 2020, many states were tasked with analyzing their state testing data across
a span of two years. Analysis of status and growth across a two-year time span was common in many states. Using approaches developed in
our work with states, we show how skip-year status and growth comparisons can be conducted in order to investigate the academic impact of
the pandemic on students.

## Missing data and changing enrollment

Due to the pandemic, numerous states experienced significant declines in student participation in state assessments. Additionally, due
to high student mobility during the pandemic, several states experienced high rates of change in terms of student enrollment.
Questions immediately emerged about what impact missing data and changing enrollment would have on comparability to 2019 results.
Comparisons between 2021 and 2019 are essential as part of any investigation into academic impact. If missing data or changing
enrollment are substantial, then those comparisons are threatened.

## Non-standardized testing situations

The pandemic forced states and assessment vendors to relax rigid test administration standards in order to collect achievement data.
Student receiving instruction from home, for example, took interim assessments from home. And in some states hardest hit by the
pandemic, students were administered the state summative assessment at home. Non-standardized testing conditions bring into question
comparability and the validity of the overall scores.

Participants in the proposed training session will be introduced to these three, real world data scenarios and instructed how they can go
about modeling the results to derive information useful as part of validation efforts and secondary data investigations that states are
keen to conduct.

Participants, utilizing the R software environment and a custom R package designed to address these data analyses, will learn how to
set up data and apply R functions to answer numerous questions that arise due to the pandemic. Participants will come away from the
training session well prepared to analyze their own data sets with regard to the issues discussed.


## Schedule

### Background: April 10, 1 to 2:00 pm ([@Dbetebenner](Damian Betebenner))

During the first two hours participants will be given an overview of the assessment data landscape that exists due to the pandemic. The pandemic 
caused numerous disruptions and alterations to student testing and we want to familiarize participants with some of the major ones (e.g., 
cancelled testing). During these two hours participants will install the required R packages necessary to perform analyses conducted throughout 
the rest of the session.

### Academic Impact (Part 1): April 10, 2:00 to 2:50 pm (Damian Betebenner)

Investigating pandemic related academic impact on student learning. During the third and fourth hours, participants will be introduced to several ways
to investigate the academic impact students encountered due to the pandemic. Including: Skip year baseline referenced growth analyses.
Status based methods of looking at impact based upon propensity score matching Andrew Hoâ€™s Fair Trend method as used to look at academic impact.
Using a toy data set that mimic 2020 test cancellations, students will learn to calculate academic impact and use those results to investigate impact by
demographic subgroups.

### Break: April 10, 2:50 to 3:00

### Missing Data (Part 1): April 11, 3:00 to 4:30 pm (Nathan Dadey)

Descriptive examination of missing data patterns. 
Multiple Imputation and Propensity Score Matching: A substantial issue associated with assessment data from 2021 was whether aggregate results
(e.g., school level results) could be compared to previous year due to missing data (non-tested students) and changing enrollment. As part of
our work with states we developed numerous multiple imputation procedures to help understand missing data as well as propensity score matching
procedures to accommodate changing enrollment. Students will learn about these procedures and use example data to see how missing data can
interfere with inferences one makes from assessment data.

### Summary and next steps: April 11, 4:30 to 5:00 pm (Damian Betebenner)
 

### Missing Data (Part 2): April 11, 1 to 2:50 pm (Adam VanIwaarden)

Multiple Imputation: A substantial issue associated with assessment data from 2021 was whether aggregate results
(e.g., school level results) could be compared to previous year due to missing data (non-tested students) and changing enrollment. As part of
our work with states we developed numerous multiple imputation procedures to help understand missing data as well as propensity score matching
procedures to accommodate changing enrollment. Students will learn about these procedures and use example data to see how missing data can
interfere with inferences one makes from assessment data.

### Break: April 11, 2:50 to 3:00

### Academic Impact (Part 2): April 11, 3 to 4:30 pm (Damian Betebenner)


### Wrap-up/Q&A: April 11, 4:30 to 5:00 pm
